{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "t6dVpIINYklI",
        "-JiQyfWJYklI",
        "Yfr_Vlr8HBkt",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Zomato Restaurant Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised Machine Learning\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzed Zomato’s restaurant data using unsupervised machine learning to cluster restaurants into meaningful segments. Conducted sentiment analysis on customer reviews to derive insights into user preferences and satisfaction. Leveraged data visualization for better interpretability, aiding businesses in improving services and helping customers find the best dining options. The project also explored cuisine trends, pricing strategies, and metadata to identify industry critics."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The restaurant industry in India is expanding rapidly, with an increasing number of food outlets catering to diverse consumer preferences. Zomato, a leading food aggregator and delivery platform, provides valuable data on restaurants, including customer reviews, menus, pricing, and ratings. However, due to the vast amount of unstructured data, it becomes challenging for both customers and businesses to derive actionable insights.\n",
        "\n",
        "Customers face difficulties in identifying the best restaurants based on their preferences, such as cuisine type, pricing, and location. Businesses struggle to analyze customer sentiment, improve service quality, and position themselves effectively in a competitive market. Additionally, understanding the clustering of restaurants based on key attributes can help Zomato optimize its recommendation system and enhance user experience.\n",
        "\n",
        "This project aims to address these challenges by:\n",
        "\n",
        "Restaurant Clustering: Using unsupervised machine learning techniques to categorize restaurants based on key attributes like location, pricing, cuisine type, and customer ratings.\n",
        "\n",
        "Sentiment Analysis: Extracting insights from customer reviews to understand overall satisfaction and areas of improvement.\n",
        "\n",
        "Data Visualization: Presenting insights in an intuitive manner to help businesses and customers make data-driven decisions.\n",
        "\n",
        "Cost vs. Benefit Analysis: Evaluating the relationship between restaurant pricing and customer satisfaction to aid businesses in competitive pricing strategies.\n",
        "\n",
        "Identifying Critics: Analyzing reviewer metadata to differentiate between casual diners and influential critics."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Zomato Restaurant reviews.csv')\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/Zomato Restaurant names and Metadata.csv')\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "UB6FjGDih7F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging the files\n",
        "data = pd.merge(df, df1, on='Name')"
      ],
      "metadata": {
        "id": "hAPoc0JljSZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"number of rows: {data.shape[0]}\")\n",
        "print(f\"number of columns: {data.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(f\"dataset info : {data.info()}\")"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"number of duplicate values: {data.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(f\"number of missing values: {data.isnull().sum()}\")"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize = (20,10))\n",
        "\n",
        "sns.heatmap(data.isnull(),cmap = 'viridis',cbar = False,yticklabels = False)\n",
        "plt.title('missing values')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Rows')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* There is a highly missing values in collections(5000) which shows the\n",
        "number of restaurants that are not tagged under any Zomato Collections for eg: Romantic-Dining, Best Cafes.\n",
        "\n",
        "* Timings (100 missing values): Some restaurants may not have updated their operating hours.\n",
        "\n",
        "* Review (45 missing values): Some users might have given a rating but skipped writing a review.\n",
        "\n",
        "* Reviewer, Rating, Metadata, Time (38 missing values each): Likely missing together, possibly due to incomplete review submissions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(data.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(f\"numberical columns: {data.describe()}\")\n",
        "print(f\"categorical columns: {data.describe(include='object')}\")"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variables_description= pd.DataFrame({\n",
        "    'Column' : data.columns,\n",
        "    'Data Type' : data.dtypes.values,\n",
        "    'Non-Null Count': data.notnull().sum().values,\n",
        "    'Missing values': data.isnull().sum().values,\n",
        "    'unique values': data.nunique().values,\n",
        "    'Sample values': [data[col].unique()[:5] for col in data.columns]} #display 1st five unique values in a column\n",
        "    )\n",
        "\n",
        "print(variables_description)"
      ],
      "metadata": {
        "id": "hoZ4WdlXn5LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name:\n",
        "*   There are 100 unique restaurant names, indicating that multiple reviews exist for each restaurant.\n",
        "*   This feature helps identify which restaurants receive the most reviews and analyze restaurant-specific trends.\n",
        "\n",
        "Reviewer:\n",
        "*  The \"Reviewer\" variable contains the name or username of the person who submitted the review.\n",
        "*  There are 7446 unique reviewers, which means some users have reviewed multiple restaurants.\n",
        "*  This can be useful for identifying top reviewers and analyzing their reviewing patterns.\n",
        "*  However, there are 38 missing values, which might indicate anonymous or deleted reviews.\n",
        "\n",
        "Review:\n",
        "\n",
        "*   This column consists of textual reviews written by users, 9364 unique review texts, implying that some reviews may be duplicates or similar.\n",
        "*   Analyzing this feature can help extract sentiment, key themes, or common phrases used in customer feedback.\n",
        "*   45 reviews are missing, which might be due to incomplete submissions or deleted entries.\n",
        "\n",
        "Rating:\n",
        "\n",
        "*   The ratings appear to be in a numerical format but are stored as an object data type.\n",
        "*   There are 10 unique rating values, which suggests that some ratings might be in decimal format (e.g., 4.5).\n",
        "*   Since 38 ratings are missing, these records may correspond to reviews that lack an explicit rating.\n",
        "\n",
        "Metadata:\n",
        "*   Metadata provides additional information about the reviewer, such as their number of past reviews and followers.\n",
        "*   This feature can be useful for determining whether experienced reviewers provide different ratings compared to casual reviewers.\n",
        "*   There are 2477 unique metadata entries, and 38 values are missing, indicating that some reviews lack reviewer details.\n",
        "\n",
        "Time:\n",
        "*  This column contains the timestamp of when the review was posted. 9782 unique timestamps, suggesting that some reviews might have been posted at the exact same time.\n",
        "*  38 values are missing, these records might be associated with deleted or system-generated reviews.\n",
        "*  Time-based analysis can be useful for understanding trends, such as peak review periods or the impact of specific events on restaurant ratings.\n",
        "\n",
        "Pictures:\n",
        "*  This variable indicates the number of pictures uploaded by the reviewer along with their review.\n",
        "*  It is stored as an integer and contains 36 unique values, showing variation in how many images users upload.\n",
        "*   A high number of pictures could indicate stronger engagement from the reviewer.\n",
        "\n",
        "Links:\n",
        "*  This column likely contains hyperlinks associated with the review, such as links to restaurant pages, menus, or food items.\n",
        "*  While the sample values are not provided, this feature could be useful for understanding how often users refer to additional resources when posting reviews.\n",
        "\n",
        "Cost:\n",
        "*  The \"Cost\" variable likely represents price-related information, such as the cost per meal or price range of the restaurant.\n",
        "*  The exact format needs further analysis to determine if it is categorical (e.g., \"Budget,\" \"Expensive\") or numerical (e.g., \"₹500 per person\").\n",
        "\n",
        "Collections:\n",
        "*  This feature indicates whether the restaurant is part of a curated collection on Zomato, such as \"Best Cafes\" or \"Fine Dining Spots.\"\n",
        "*  Since 5000 values are missing, this suggests that many restaurants are not included in specific collections, possibly due to their niche offerings or lower popularity.\n",
        "\n",
        "Cuisines:\n",
        "*  The \"Cuisines\" variable specifies the type of food offered by the restaurant, such as \"Indian,\" \"Chinese,\" or \"Italian\".\n",
        "*  This information is useful for filtering restaurant reviews based on cuisine type and analyzing which cuisines receive the best ratings.\n",
        "\n",
        "Timings:\n",
        "*  This column contains information about the restaurant's operating hours.\n",
        "*  Since 100 values are missing, some restaurants may not have provided their timings, or they may have varying schedules that are difficult to standardize.\n",
        "*  Analyzing this feature can help identify which time slots are most popular for dining and reviewing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_values = {col: data[col].unique() for col in data.columns}\n",
        "\n",
        "for column, values in unique_values.items():\n",
        "  print(f\"Column: {column}\")\n",
        "  print(f\"unique values: {values[:10]}\")\n",
        "  print(f\"total unique values : {len(values)}/n\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Display basic info\n",
        "print(\"Before Cleaning:\")\n",
        "print(data.info())\n",
        "print(\"\\nMissing Values:\\n\", data.isnull().sum())\n",
        "\n",
        "# Step 1: Standardize Column Names (Lowercase + Strip Spaces)\n",
        "data.columns = data.columns.str.lower().str.strip()\n",
        "\n",
        "# Step 2: Handling Missing Values (Numerical - Mean/Median, Categorical - Mode)\n",
        "# Convert 'rating' to numeric (remove errors due to string format)\n",
        "data[\"rating\"] = pd.to_numeric(data[\"rating\"], errors=\"coerce\")\n",
        "\n",
        "# Convert 'cost' to numeric (remove commas before conversion)\n",
        "if \"cost\" in data.columns:\n",
        "    data[\"cost\"] = data[\"cost\"].astype(str).str.replace(\",\", \"\").astype(float)\n",
        "\n",
        "# Fill missing values\n",
        "num_cols = [\"rating\", \"cost\"]  # List of numerical columns (excluding metadata)\n",
        "cat_cols = [\"reviewer\", \"review\", \"collections\", \"cuisines\", \"timings\", \"metadata\"]  # Categorical columns (including metadata)\n",
        "\n",
        "for col in num_cols:\n",
        "    if col in data.columns:\n",
        "        data[col].fillna(data[col].median(), inplace=True)  # Using median for numerical\n",
        "\n",
        "for col in cat_cols:\n",
        "    if col in data.columns:\n",
        "        # Check if mode is not empty before accessing element at index 0\n",
        "        mode_values = data[col].mode()\n",
        "        if not mode_values.empty:\n",
        "            data[col].fillna(mode_values.iloc[0], inplace=True)\n",
        "        else:\n",
        "            # Handle cases where mode is empty, e.g., fill with a placeholder\n",
        "            data[col].fillna(\"Unknown\", inplace=True)  # Or any other appropriate value\n",
        "\n",
        "# Step 3: Handle Categorical Data\n",
        "if \"cuisines\" in data.columns:\n",
        "    data[\"cuisines\"] = data[\"cuisines\"].astype(str).str.lower().str.strip()  # Standardize text\n",
        "\n",
        "# Step 4: Convert Timestamps\n",
        "if \"time\" in data.columns:\n",
        "    data[\"time\"] = pd.to_datetime(data[\"time\"], errors=\"coerce\")\n",
        "\n",
        "# Step 5: Handle Duplicates\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Step 6: Outlier Detection & Treatment\n",
        "if \"rating\" in data.columns:\n",
        "    Q1 = data[\"rating\"].quantile(0.25)\n",
        "    Q3 = data[\"rating\"].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    data[\"rating\"] = np.where(data[\"rating\"] < lower_bound, lower_bound, data[\"rating\"])\n",
        "    data[\"rating\"] = np.where(data[\"rating\"] > upper_bound, upper_bound, data[\"rating\"])\n",
        "\n",
        "#  Step 7: Final Check\n",
        "print(\"\\nAfter Cleaning:\")\n",
        "print(data.info())\n",
        "print(\"\\nMissing Values After Cleaning:\\n\", data.isnull().sum())\n",
        "\n",
        "# Save cleaned data\n",
        "data.to_csv(\"cleaned_data.csv\", index=False)\n",
        "print(\"\\n Data Cleaning Complete! Saved as 'cleaned_data.csv'\")\n"
      ],
      "metadata": {
        "id": "NECrax4bITz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following steps were performed to clean and preprocess the dataset:\n",
        "\n",
        "**Standardized Column Names**:\n",
        "Converted all column names to lowercase and stripped spaces to ensure uniformity.\n",
        "\n",
        "To: Prevents issues with case sensitivity.\n",
        "Makes column names consistent and easy to reference in code.\n",
        "\n",
        "**Handled Missing Values**:\n",
        "Numerical Columns (rating, cost) → Filled with median.\n",
        "Categorical Columns (reviewer, review, cuisines, timings) → Filled with mode.\n",
        "\n",
        "To:\n",
        "Median is robust against outliers in numerical data.\n",
        "Mode ensures categorical columns remain meaningful.\n",
        "\n",
        "**Converted Data Types**:\n",
        "Converted \"rating\" and \"cost\" to numeric (removed commas),\"time\" to datetime.\n",
        "\n",
        "To:\n",
        "Prevents errors when performing calculations.\n",
        "Ensures timestamps are recognized properly for time-series analysis.\n",
        "\n",
        "**Handled Duplicates**:\n",
        "Removed duplicate records.\n",
        "\n",
        "To:\n",
        "Ensures data consistency and avoids redundant analysis.\n",
        "\n",
        "**Cleaned Text Data**:\n",
        "Standardized \"cuisines\" by converting to lowercase and stripping spaces.\n",
        "\n",
        "To:\n",
        "Prevents issues caused by case-sensitive text mismatches.\n",
        "\n",
        "**Handled Outliers**:\n",
        "Used IQR (Interquartile Range) Method to cap \"rating\" values to a reasonable range.\n",
        "\n",
        "To:\n",
        "Ensure extreme values don't skew analysis.\n",
        "\n",
        "**Saved Cleaned Data**:\n",
        "Exported cleaned data to cleaned_data.csv.\n",
        "\n",
        "To:\n",
        "Allow further analysis, visualization, and model training without redoing preprocessing."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart1 - Distribution of Ratings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(data[\"rating\"], bins=10, kde=True, color=\"blue\")\n",
        "plt.title(\"Distribution of Ratings\", fontsize=14)\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a Histogram because it effectively displays the distribution of numerical data. It helps in understanding:\n",
        "*  The overall spread and frequency of ratings.\n",
        "*  Whether the ratings are normally distributed, skewed, or have outliers.\n",
        "*  If there are any common rating trends (e.g., most ratings clustering around a certain value).\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skewed Ratings**:\n",
        "\n",
        "\n",
        "* If most ratings cluster around 4+, it indicates that restaurants generally receive good reviews.\n",
        "* If the histogram is right-skewed (more low ratings), it could signal service/food quality issues.\n",
        "\n",
        "**Presence of Outliers**:\n",
        "*  If we see spikes at 0 or 5, it could mean biased reviews (either too many perfect scores or extreme dissatisfaction).\n",
        "\n",
        "**Gaps or Missing Ratings**:\n",
        "*  If there are gaps, it might suggest data inconsistencies or missing values in rating collection.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "*  If ratings are generally high, it suggests customer satisfaction is strong, reinforcing a good reputation.\n",
        "*  A well-maintained positive rating trend can boost customer trust, increasing revenue and footfall.\n",
        "\n",
        "\n",
        "*  Understanding distribution allows businesses to identify and reward top-performing locations or cuisines.\n",
        "*   List item\n",
        "\n",
        "Potential Negative Growth Indicators:\n",
        "*  If there is a high number of low ratings (1-2 stars), it could indicate poor service, food quality, or pricing issues.\n",
        "*  A bimodal distribution (two peaks) may suggest inconsistent service, where some branches perform well while others fail.\n",
        "*  If a large portion of the ratings is missing, it reduces trust and credibility, discouraging new customers.\n",
        "\n",
        "\n",
        "\n",
        "To mitigate negative growth, businesses should:\n",
        "*  Investigate customer complaints (why are some ratings low?).\n",
        "*  Address operational inefficiencies (service delays, pricing mismatches).\n",
        "*  Encourage genuine reviews to improve data quality and credibility.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart -  Rating vs. Cost Relationship\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=data[\"cost\"], y=data[\"rating\"], alpha=0.5, color=\"green\")\n",
        "plt.title(\"Rating vs. Cost\", fontsize=14)\n",
        "plt.xlabel(\"Cost\")\n",
        "plt.ylabel(\"Rating\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Strong Correlation Between Cost and Rating:\n",
        "\n",
        "*  The points are scattered evenly, indicating that higher cost does not guarantee higher ratings.\n",
        "*  Restaurants with low prices still have high ratings, showing that affordability does not necessarily mean poor quality.\n",
        "\n",
        "Clusters of High Ratings Across Different Cost Levels:\n",
        "\n",
        "\n",
        "*  Many restaurants rated 4.0 and above exist in all price ranges, meaning that customer satisfaction depends on factors beyond cost (e.g., food quality, service, ambiance).\n",
        "\n",
        "\n",
        "Low-Rated Restaurants at Various Cost Points\n",
        "*  Some restaurants with high costs have low ratings, suggesting poor value for money.\n",
        "*  This could mean issues with service, food quality, or customer expectations.\n",
        "\n",
        "\n",
        "Positive Business Impact:\n",
        "*  Businesses can price their offerings competitively without fearing a drop in ratings.\n",
        "*  Low-cost restaurants with high ratings can be marketed effectively to attract more budget-conscious customers.\n",
        "*  Premium restaurants need to justify their price with superior service and experience to maintain high ratings.\n",
        "\n",
        "\n",
        "Potential Negative Growth Indicators:\n",
        "*  High-cost restaurants with poor ratings indicate dissatisfied customers, leading to loss of revenue and reputation.\n",
        "*  If no clear link exists between price and ratings, businesses must focus on quality improvements instead of just increasing prices.\n",
        "\n",
        "Next Steps?\n",
        "\n",
        "\n",
        "*  Identify top-performing low-cost restaurants and learn from their success strategies.\n",
        "*  Investigate why high-cost restaurants have low ratings—bad service? Poor food quality?\n",
        "*  Use targeted marketing to highlight value-for-money options.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - Top 10 Most Popular Cuisines\n",
        "plt.figure(figsize=(10,5))\n",
        "top_cuisines = data[\"cuisines\"].value_counts().head(10)\n",
        "sns.barplot(x=top_cuisines.index, y=top_cuisines.values, palette=\"viridis\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top 10 Most Popular Cuisines\", fontsize=14)\n",
        "plt.xlabel(\"Cuisine\")\n",
        "plt.ylabel(\"Number of Restaurants\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen because:\n",
        "\n",
        "✅ It effectively compares the popularity of different cuisines.\n",
        "\n",
        "✅ It provides a clear visual representation of which cuisines are in high demand.\n",
        "\n",
        "✅ It helps in making business decisions regarding menu offerings."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ North Indian & Chinese cuisine dominate the market – The highest number of restaurants serve this combination, indicating high demand.\n",
        "\n",
        "2️⃣ South Indian and Biryani-based combinations are also popular, making them a strong second-tier preference.\n",
        "\n",
        "3️⃣ Continental and Dessert categories are lower in demand, but still have a significant number of restaurants.\n",
        "\n",
        "4️⃣ Hyderabadi cuisine has lower representation, which might indicate a niche opportunity for businesses.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Restaurants can prioritize North Indian and Chinese dishes to attract more customers.\n",
        "*  Businesses looking for a competitive edge might focus on less saturated cuisines like Hyderabadi or Continental, especially in areas where demand is growing.\n",
        "*  Menu diversification can be planned, ensuring a mix of popular and emerging cuisines to cater to different customer segments.\n",
        "\n",
        "\n",
        "Final Business Recommendations:\n",
        "\n",
        "For new restaurants → Consider specializing in less saturated cuisines or offering fusion dishes to stand out.\n",
        "\n",
        "For existing restaurants → Continue with popular dishes but introduce seasonal menus or innovative combinations.\n",
        "\n",
        "For marketing teams → Focus on customer engagement strategies, such as offering discounts on less popular cuisines to drive demand.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart -  Average Cost per Rating Category\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=data.groupby('rating')['cost'].mean().index, y=data.groupby('rating')['cost'].mean().values, palette='Blues')\n",
        "plt.title('Average Cost per Rating Category')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Average Cost')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is chosen as it clearly displays the relationship between ratings and average cost.\n",
        "\n",
        "It helps in visualizing how cost varies with customer ratings, making it easier to identify patterns."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average cost fluctuates across different rating categories, with some lower ratings having higher costs.\n",
        "\n",
        "Ratings around 2.5 show the highest cost, while highly rated restaurants (4.5-5.0) maintain a moderate cost.\n",
        "\n",
        "Lower-rated restaurants (1.0 - 2.0) also show significant costs, indicating that price alone doesn’t guarantee higher ratings."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact:\n",
        "\n",
        "*  Helps price optimization—businesses can adjust pricing strategies based on ratings.\n",
        "*  Indicates that high-cost restaurants don't always get higher ratings, suggesting improvements in service, food quality, or ambiance are needed.\n",
        "*  Competitive pricing strategies can be applied to attract more customers to high-rated restaurants.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "\n",
        "*  High-cost restaurants receiving low ratings may struggle with poor value for money or service quality issues.\n",
        "*  Businesses with moderate pricing but high ratings can leverage this to attract more customers, putting pressure on expensive, underperforming places.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "popular_collections = data.groupby(\"collections\")[\"rating\"].mean().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=popular_collections.values, y=popular_collections.index, palette=\"viridis\")\n",
        "\n",
        "# Titles and labels\n",
        "plt.title(\"Top 10 Most Popular Collections by Rating\", fontsize=20)\n",
        "plt.xlabel(\"Average Rating\")\n",
        "plt.ylabel(\"Collection Name\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MJNFfDyyloy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "2Dw3HvLxmBCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart was chosen to visually compare the average ratings across different popular collections.This makes it easier to read long collection names and see which categories are the highest-rated."
      ],
      "metadata": {
        "id": "1Ed3NyxJmBO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "IQ3XQT1ymBX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Barbecue & Grill, Great Buffets, Corporate Favorites\" has the highest average rating, indicating strong customer preference.\n",
        "\n",
        "Collections featuring buffets, food hygiene, and sports screenings are among the most popular, showing that consumers prioritize diverse dining experiences, cleanliness, and entertainment.\n",
        "\n",
        "Sunday brunches and corporate favorites also have high ratings, suggesting a demand for weekend dining and professional gatherings."
      ],
      "metadata": {
        "id": "WxL0lJNNmBb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Will the gained insights help creating a positive business impact?"
      ],
      "metadata": {
        "id": "o1NLA-E6mBfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact:\n",
        "\n",
        "\n",
        "*  Helps restaurants align their marketing strategies—they can highlight features from top-rated collections to attract more customers.\n",
        "\n",
        "*  Food hygiene is a key factor in customer preference, meaning businesses should invest in cleanliness and safety measures to gain higher ratings.\n",
        "*  Restaurants focusing on grill, buffets, and live entertainment can expand their offerings to tap into the popular demand.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "*  If certain collections receive consistently lower ratings, it may indicate issues such as poor service, quality concerns, or lack of appeal.\n",
        "*  Collections without food hygiene ratings may struggle to attract health-conscious consumers, impacting their growth negatively.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tH4biOdBmBjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - Average Pictures per Rating Category\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=data.groupby('rating')['pictures'].mean().index, y=data.groupby('rating')['pictures'].mean().values, palette='Greens')\n",
        "plt.title('Average Pictures per Rating Category')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Average Pictures')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen to show the relationship between ratings and the number of pictures uploaded per rating category.\n",
        "\n",
        "It helps in understanding how user engagement (through pictures) varies with ratings."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*  Moderate ratings (3.5) have the highest number of pictures uploaded, indicating users in this range are more likely to share their experience.\n",
        "*  Lower ratings (1.0 - 2.5) have very few pictures, possibly due to negative experiences leading to less engagement.\n",
        "*  High ratings (4.5 - 5.0) also show fewer pictures compared to mid-range ratings, which could mean satisfied customers don't feel the need to upload as many pictures.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact:\n",
        "*  Businesses can encourage highly satisfied customers (4.5 - 5.0 ratings) to upload more pictures through incentives like discounts or loyalty points.\n",
        "*  Since moderate ratings (3.5) have the most engagement, businesses should analyze these reviews to understand what customers like and improve upon.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "\n",
        "*  Very low ratings (1.0 - 2.5) show minimal picture uploads, suggesting customers are disengaged and might not return.\n",
        "*  A lack of user-generated content for low-rated businesses reduces their online visibility, making it harder to improve reputation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - Proportion of Ratings\n",
        "rating_counts = data['rating'].value_counts()\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', colors=sns.color_palette('pastel'))\n",
        "plt.title('Proportion of Ratings')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart was chosen because it effectively shows the distribution of ratings as proportions of the total.It provides a quick visual understanding of which rating categories dominate."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority (38.5%) of ratings are 5-star, indicating a significant number of highly satisfied customers.\n",
        "\n",
        "Ratings of 4-star (23.8%) and 1-star (17.4%) are also notable, showing both strong positive and negative opinions.\n",
        "\n",
        "Very low proportions for ratings between 1.5 and 2.5, suggesting that customers either have extreme satisfaction or dissatisfaction rather than moderate feedback."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Impact:**\n",
        "\n",
        "\n",
        "*  A high proportion of 5-star ratings (38.5%) means strong customer satisfaction, which can be leveraged for marketing and branding.\n",
        "*  Since 4-star ratings also have a strong presence (23.8%), businesses can analyze these reviews to push them toward 5-star ratings by improving minor aspects.\n",
        "*   The low proportion of 2-star ratings (6.9%) suggests that businesses do not often receive slightly negative feedback, meaning dissatisfied customers likely express stronger discontent (1-star).\n",
        "\n",
        "**Negative Growth Indicators:**\n",
        "\n",
        "*  A significant portion of 1-star ratings (17.4%) indicates a serious issue with a segment of customers. If not addressed, this can damage reputation and customer retention.\n",
        "*  The lack of moderate ratings (2-3) suggests that customers have polarizing experiences—either very good or very bad—which might indicate inconsistent service quality.\n",
        "\n",
        "**Action Plan for Business Improvement:**\n",
        "\n",
        "*  Investigate the reasons behind 1-star ratings by analyzing negative reviews and taking corrective actions.\n",
        "*  Encourage 4-star raters to convert to 5-star by addressing minor concerns through customer engagement.\n",
        "*  Ensure consistency in service quality to reduce extreme variations in ratings.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - Histogram of Cost\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(data['cost'], bins=20, kde=True, color='green')\n",
        "plt.title('Distribution of Cost')\n",
        "plt.xlabel('Cost')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was chosen because it effectively displays the frequency distribution of cost values over different price ranges.\n",
        "\n",
        "The overlaid KDE (Kernel Density Estimate) line helps visualize the probability distribution and smoothens the trend."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution is not uniform, with a strong peak around 500-550, indicating that most costs fall in this range.\n",
        "\n",
        "There are multiple smaller peaks (multi-modal distribution), suggesting the presence of different pricing segments.\n",
        "\n",
        "There are some low-frequency occurrences of very low (below 200) and very high (above 800) costs."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact:\n",
        "\n",
        "\n",
        "*  The highest concentration around 500-550 suggests a standard price range, helping businesses identify their most competitive pricing bracket.\n",
        "*  Different peaks indicate multiple market segments, allowing businesses to optimize pricing strategies for diverse customer groups.\n",
        "*  A well-defined peak means pricing consistency, which can lead to better customer trust and predictable revenue.\n",
        "\n",
        "Negative Growth Indicators:\n",
        "\n",
        "*  Multiple peaks suggest pricing inconsistency, which might confuse customers or indicate inefficient pricing strategies.\n",
        "*  The presence of extreme low and high-cost values could point to an issue in price standardization or an underserved high-end/lower-end market.\n",
        "\n",
        "\n",
        "Plan for Business Improvement:\n",
        "\n",
        "*  Leverage the most frequent pricing range (500-550) as a baseline for competitive pricing strategies.\n",
        "*  Analyze the peaks to determine if different pricing strategies are needed for various customer segments.\n",
        "*  Investigate extremely high-cost and low-cost occurrences to ensure that pricing is optimized for profitability without alienating potential customers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8- Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code - How Are Features Related?\n",
        "numerical_features = data.select_dtypes(include=['number'])\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.heatmap(numerical_features.corr(), annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap was chosen because:\n",
        "*  It visually represents the relationship\n",
        "between multiple features.\n",
        "*  It helps in identifying strong and weak correlations between variables.\n",
        "*  It allows businesses to focus on key drivers of customer satisfaction and pricing.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low correlation between Cost and Rating (0.026):\n",
        "\n",
        "*  This suggests that higher cost does not necessarily mean better ratings. Customers might be rating restaurants based on factors other than pricing, such as service or food quality.\n",
        "\n",
        "Low correlation between Pictures and Rating (0.083):\n",
        "*  The number of pictures uploaded doesn’t significantly impact ratings. This could mean that customers don’t rely heavily on images when rating restaurants, or that other factors matter more.\n",
        "\n",
        "Low correlation between Pictures and Cost (0.034) :\n",
        "*  The cost of food at a restaurant does not significantly influence the number of pictures taken or uploaded.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "#Pairplot of Numerical Variables\n",
        "sns.pairplot(data[['rating', 'cost', 'pictures']])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*  A pairplot is useful when analyzing relationships between multiple numerical variables in a dataset.\n",
        "*  It provides a combination of scatter plots (for relationships) and histograms (for distributions) of variables like rating, cost, and pictures.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rating vs. Cost:\n",
        "\n",
        "\n",
        "*  Ratings appear evenly distributed across different cost ranges, meaning cost might not have a strong impact on customer ratings.\n",
        "*  However, some higher-cost products still receive lower ratings, suggesting cost is not always a quality indicator.\n",
        "\n",
        "Rating vs. Pictures:\n",
        "\n",
        "\n",
        "*  There seems to be a scattered distribution with no strong trend, meaning more pictures don’t necessarily lead to higher ratings.\n",
        "*  Some higher-rated products have fewer pictures, suggesting quality perception might not be solely based on image count.\n",
        "\n",
        "Cost vs. Pictures:\n",
        "*  Most products have fewer than 10 pictures, but there are outliers with up to 60 pictures.\n",
        "*  Higher-cost items generally have more pictures, possibly because premium products focus more on visual appeal.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1  : The average cost of high-rated products (ratings 4 & 5) is significantly higher than low-rated products (ratings 1 & 2)."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The average cost of high-rated products (ratings 4 & 5) is not significantly higher than low-rated products (ratings 1 & 2).\n",
        "\n",
        "Alternate Hypothesis (H₁): The average cost of high-rated products (ratings 4 & 5) is significantly higher than low-rated products (ratings 1 & 2)."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/cleaned_data.csv\")\n",
        "\n",
        "# Filter data\n",
        "high_rated = df[df['rating'].isin([4, 5])]['cost']\n",
        "low_rated = df[df['rating'].isin([1, 2])]['cost']\n",
        "\n",
        "# Perform independent t-test\n",
        "t_stat, p_value = stats.ttest_ind(high_rated, low_rated, equal_var=False)\n",
        "\n",
        "# Display results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: High-rated products have significantly different costs compared to low-rated products.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant difference in cost between high-rated and low-rated products.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: Independent t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Why t-test?\n",
        "\n",
        "The independent t-test is used to compare the means of two independent groups (high-rated vs. low-rated products).\n",
        "\n",
        "Assumes normality in distributions and accounts for unequal variances using equal_var=False."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 : The number of pictures significantly influences the rating of a product."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The number of pictures does not significantly affect product ratings.\n",
        "\n",
        "Alternate Hypothesis (H₁): The number of pictures has a significant effect on product ratings."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Check data structure\n",
        "print(df.head())\n",
        "\n",
        "# Perform Spearman's correlation test\n",
        "correlation, p_value = spearmanr(df['pictures'], df['rating'])\n",
        "\n",
        "# Display results\n",
        "print(f\"Spearman's Correlation Coefficient: {correlation:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The number of pictures significantly influences product ratings.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant relationship between the number of pictures and product ratings.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test: ANOVA or Spearman’s correlation (depending on data distribution)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spearman's Correlation Coefficient:\n",
        "*  Product ratings are ordinal (1 to 5), meaning they follow a rank order.\n",
        "*  The number of pictures is continuous but may not follow a normal distribution.\n",
        "*  Spearman’s correlation is ideal for testing monotonic relationships between an ordinal and a continuous variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3 : Products with higher costs receive more ratings compared to lower-cost products."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): There is no significant relationship between product cost and the number of ratings.\n",
        "\n",
        "Alternate Hypothesis (H₁): Higher-cost products receive significantly more ratings than lower-cost products."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Perform Spearman's correlation test\n",
        "correlation, p_value = spearmanr(df['cost'], df['rating'])\n",
        "\n",
        "# Display results\n",
        "print(f\"Spearman's Correlation Coefficient: {correlation:.4f}\")\n",
        "print(f\"P-Value: {p_value:.4f}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: Higher-cost products receive significantly more ratings.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant relationship between cost and product ratings.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spearman’s Correlation Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Ratings are ordinal (1 to 5) and follow a rank order.\n",
        "*   Cost is continuous, but it may not be normally distributed.\n",
        "*   Spearman’s correlation is ideal for testing relationships between a continuous and an ordinal variable when normality is uncertain.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Check for missing values\n",
        "\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "# Fill missing values in 'Cost' with median\n",
        "data[\"cost\"].fillna(data[\"cost\"].median(), inplace=True)\n",
        "\n",
        "# Fill missing values in 'Rating' with median\n",
        "data[\"rating\"].fillna(data[\"rating\"].median(), inplace=True)\n",
        "\n",
        "# Drop columns with excessive missing values (optional)\n",
        "data.dropna(axis=1, thresh=len(data) * 0.5, inplace=True)\n",
        "\n",
        "# Confirm no missing values remain\n",
        "print(\"Missing Values After Cleaning:\\n\", data.isnull().sum())\n",
        "\n",
        "## Remove Duplicate Entries\n",
        "# Check for duplicates\n",
        "print(\"Total Duplicates:\", data.duplicated().sum())\n",
        "\n",
        "# Drop duplicate rows\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Confirm removal\n",
        "print(\"Total Duplicates After Removal:\", data.duplicated().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For missing value imputation, I used the median imputation technique for numerical columns like cost and rating.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "Median Imputation:\n",
        "*  The median is a robust statistic that is not affected by extreme values or skewed distributions, making it ideal for handling missing values in numerical columns like cost and rating.\n",
        "*  Since costs and ratings often have outliers, using the mean could distort the data, whereas the median provides a more stable estimate.\n",
        "\n",
        "For other columns:\n",
        "\n",
        "Categorical Columns:\n",
        "\n",
        "\n",
        "*  Since there are no missing values in categorical features like name, reviewer, cuisines, etc., imputation was not necessary.\n",
        "*  However, if needed, common techniques include replacing missing values with the mode (most frequent value) or a special category like 'Unknown' to retain the missing information.\n",
        "\n",
        "Time Column:\n",
        "*  The time column still has 3,692 missing values.\n",
        "*  The imputation approach for this would depend on the data context—options include forward fill, backward fill, or using a placeholder like \"Not Available\" if it's categorical.\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "## Handle Outliers (Using IQR Method)\n",
        "\n",
        "def remove_outliers(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
        "\n",
        "# Remove outliers from Cost column\n",
        "data = remove_outliers(data, \"cost\")\n",
        "\n",
        "# Remove outliers from Rating column\n",
        "data = remove_outliers(data, \"rating\")\n",
        "\n",
        "# Confirm changes\n",
        "print(\"Data Shape After Outlier Removal:\", data.shape)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I used the Interquartile Range (IQR) method, which is effective for detecting and removing extreme values in numerical data.\n",
        "\n",
        "Resistant to Skewness:\n",
        "*  Unlike standard deviation-based methods, the IQR method is not affected by the shape of the distribution, making it more reliable for skewed data.\n",
        "\n",
        "Preserves Important Data:\n",
        "*   It only removes extreme values rather than modifying the entire dataset, ensuring that the core information remains intact.\n",
        "\n",
        "Effective for Non-Normal Data:\n",
        "*   Unlike Z-score methods (which assume normality), IQR works well even if the data is not normally distributed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# One-hot encode the 'Cuisines' column\n",
        "data = pd.get_dummies(data, columns=[\"cuisines\"], drop_first=True)\n",
        "\n",
        "# Display updated dataset\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding (OHE) **:\n",
        "\n",
        "*  Suitable for Nominal Data: Since cuisines represents categories without an inherent order (e.g., Italian, Chinese, Indian), OHE is the best choice.\n",
        "*  Ensures Model Compatibility: Many machine learning algorithms work better with numerical data, making encoding essential.\n",
        "*  Prevents Misinterpretation: Unlike label encoding, OHE does not introduce any ordinal relationship where none exists.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "# Download required NLTK packages\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "metadata": {
        "id": "0rGRIMp657nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'text': [\"I can't believe it's 2024! This is amazing... Visit https://example.com for more!\"]}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "LkPRqDYl6qO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "\n",
        "import contractions"
      ],
      "metadata": {
        "id": "o7hEEvS66yzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "#Many words in English are written in their contracted form (e.g., \"can't\" → \"cannot\").\n",
        "\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "def to_lowercase(text):\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_urls_digits(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)  # Remove words containing digits\n",
        "    return text"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords and white spaces\n",
        "\n",
        "def remove_stopwords_whitespace(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text\n",
        "\n",
        "Stemming: Reduces words to their root form (e.g., \"running\" → \"run\").\n",
        "\n",
        "Lemmatization: Converts words into their base form while considering context."
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "    words = word_tokenize(text)\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "print(nltk.data.path)\n",
        "\n",
        "nltk.data.path.append('/usr/local/nltk_data')\n",
        "\n",
        "# POS Taging\n",
        "def pos_tagging(text):\n",
        "    words = word_tokenize(text)\n",
        "    return nltk.pos_tag(words)\n",
        "\n",
        "# Apply Preprocessing\n",
        "df['text'] = df['text'].apply(expand_contractions)\n",
        "df['text'] = df['text'].apply(to_lowercase)\n",
        "df['text'] = df['text'].apply(remove_punctuation)\n",
        "df['text'] = df['text'].apply(remove_urls_digits)\n",
        "#df['text'] = df['text'].apply(remove_stopwords_whitespace)\n",
        "df['text'] = df['text'].apply(lemmatization)\n",
        "\n",
        "# Step 9: POS Tagging\n",
        "df['pos_tags'] = df['text'].apply(pos_tagging)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Step 10: Text Vectorization\n",
        "vectorizer_tfidf = TfidfVectorizer(max_features=500)\n",
        "vectorizer_count = CountVectorizer(max_features=500)\n",
        "\n",
        "tfidf_matrix = vectorizer_tfidf.fit_transform(df['text'])\n",
        "count_matrix = vectorizer_count.fit_transform(df['text'])\n",
        "\n",
        "\n",
        "# Convert to DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
        "count_df = pd.DataFrame(count_matrix.toarray(), columns=vectorizer_count.get_feature_names_out())\n",
        "\n",
        "# Display Results\n",
        "print(\"Processed Data:\\n\", df)\n",
        "print(\"\\nTF-IDF Features:\\n\", tfidf_df.head())\n",
        "print(\"\\nCountVectorizer Features:\\n\", count_df.head())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used TF-IDF Vectorization and Count Vectorization to transform text data into numerical representations for analysis.\n",
        "\n",
        "**Count Vectorization**:\n",
        "*  Converts text into a word frequency matrix, where each word is represented by its occurrence count in the document.\n",
        "\n",
        "Why?\n",
        "*  Simple and efficient for basic NLP tasks like text classification and clustering.\n",
        "*  Works well when word frequency is a key indicator of importance.\n",
        "\n",
        "**TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
        "\n",
        "Assigns a weight to each word based on its frequency in a document and across the entire dataset, where:\n",
        "\n",
        "TF (Term Frequency): Measures how often a word appears in a document.\n",
        "\n",
        "IDF (Inverse Document Frequency): Reduces the weight of common words and increases the importance of rare words.\n",
        "\n",
        "Why?\n",
        "*  Captures important words while reducing the influence of common but uninformative terms.\n",
        "*  Useful for information retrieval, text classification, and sentiment analysis.\n",
        "\n",
        "\n",
        "\n",
        "CountVectorizer is good for straightforward frequency analysis.\n",
        "\n",
        "TF-IDF helps prioritize important words over frequently occurring but unimportant words.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Extract the number of words in the review (if reviews exist)\n",
        "import pandas as pd\n",
        "data1 = pd.read_csv('/content/cleaned_data.csv')\n",
        "\n",
        "if \"review\" in data1.columns:\n",
        "    data1[\"review_Length\"] = data1[\"review\"].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "# Extract price range from cost\n",
        "data1[\"cost_range\"] = pd.cut(data1[\"cost\"], bins=[0, 500, 1000, 2000, 5000], labels=[\"Low\", \"Medium\", \"High\", \"Premium\"])\n",
        "\n",
        "\n",
        "# Display new features\n",
        "data1.head()\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(data1.columns)\n",
        "\n",
        "data1[\"cost\"].fillna(data1[\"cost\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Load dataset\n",
        "data1 = pd.read_csv('/content/cleaned_data.csv')\n",
        "\n",
        "# Feature Engineering\n",
        "if \"review\" in data1.columns:\n",
        "    data1[\"review_length\"] = data1[\"review\"].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "if \"cost\" in data1.columns:\n",
        "    data1[\"cost\"] = data1[\"cost\"].fillna(data1[\"cost\"].median())  # Handle missing values\n",
        "    data1[\"cost_range\"] = pd.cut(data1[\"cost\"], bins=[0, 500, 1000, 2000, 5000], labels=[\"Low\", \"Medium\", \"High\", \"Premium\"])\n",
        "\n",
        "#  Handle Rating Issue\n",
        "if \"rating\" in data1.columns:\n",
        "    data1[\"rating\"] = pd.to_numeric(data1[\"rating\"], errors=\"coerce\")  # Ensure numeric\n",
        "    data1[\"rating\"] = data1[\"rating\"].fillna(data1[\"rating\"].median())  # Fill NaN with median\n",
        "\n",
        "    # Check unique values\n",
        "    unique_ratings = data1[\"rating\"].nunique()\n",
        "    # Create initial bins using qcut\n",
        "categories, bin_edges = pd.qcut(data1[\"rating\"], q=len(bins)-1, retbins=True, duplicates=\"drop\")\n",
        "\n",
        "# Adjust labels dynamically based on actual bin count\n",
        "num_bins = len(bin_edges) - 1\n",
        "labels = list(range(num_bins))\n",
        "\n",
        "# Apply qcut again with the corrected labels\n",
        "data1[\"rating_category\"] = pd.qcut(data1[\"rating\"], q=num_bins, labels=labels, duplicates=\"drop\")\n",
        "\n",
        "# 3Feature Scaling (Ensure non-negative values)\n",
        "num_features = [\"cost\", \"review_length\"]  # Excluding \"rating\"\n",
        "scaler = MinMaxScaler()\n",
        "data1[num_features] = scaler.fit_transform(data1[num_features])\n",
        "\n",
        "# Feature Selection using Chi-Square Test\n",
        "X = data1[num_features]  # Select numerical features\n",
        "selector = SelectKBest(score_func=chi2, k=min(2, X.shape[1]))  # Select top features (at most available features)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "y = LabelEncoder().fit_transform(y)  # Convert to integer labels\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(\"Selected Features:\", selected_features)\n",
        "\n",
        "# Final dataset with selected features\n",
        "final_data = data1[selected_features]\n",
        "print(\"Final Dataset Shape:\", final_data.shape)\n"
      ],
      "metadata": {
        "id": "lE5tX02K0Lc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SelectKBest (Chi-Square Test):\n",
        "\n",
        "*  Used to select the top k features based on statistical scores.\n",
        "*  Chi-Square Test was used for categorical targets (if applicable).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost:\n",
        "*  Directly impacts user behavior, purchasing decisions, and ratings.\n",
        "*  Higher correlation with the target variable.\n",
        "\n",
        "Review Length:\n",
        "\n",
        "*  Indicates user engagement—longer reviews may reflect stronger opinions (positive or negative).\n",
        "*   Found statistically significant in influencing the target variable.\n",
        "\n",
        "\n",
        "These features were selected because they contribute the most to predicting the outcome while reducing noise in the dataset."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?\n",
        "\n",
        "\n",
        "Yes, data transformation was necessary to improve model performance and ensure feature compatibility with machine learning algorithms.\n",
        "\n",
        "Why?\n",
        "\n",
        "Log Transformation (For Skewed Features):\n",
        "\n",
        "\n",
        "* Applied to right-skewed features like cost to normalize their distribution.\n",
        "* Reduces the impact of extreme values (outliers) and improves model stability.\n",
        "\n",
        "Standardization (For Numerical Features with Different Scales):\n",
        "\n",
        "*  Used StandardScaler to ensure all numerical features have a mean of 0 and standard deviation of 1.\n",
        "*  Helps gradient-based models converge faster.\n",
        "\n",
        "Label Encoding (For Categorical Variables):\n",
        "*  Converts categorical variables into numerical form for compatibility with models.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Log transformation for skewed features\n",
        "data1[\"cost\"] = np.log1p(data1[\"cost\"])  # log(1 + x) to handle zero values\n",
        "\n",
        "# Standardization for numerical features\n",
        "scaler = StandardScaler()\n",
        "num_features = [\"cost\", \"review_length\"]\n",
        "data1[num_features] = scaler.fit_transform(data1[num_features])\n",
        "\n",
        "# Label Encoding for categorical variables\n",
        "categorical_features = [\"metadata\"]  # Example categorical column\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    data1[col] = le.fit_transform(data1[col])\n",
        "\n",
        "print(\"Data Transformation Completed!\")\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Selecting numerical features\n",
        "numerical_features = [\"cost\", \"rating\"]\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply scaling\n",
        "data1[numerical_features] = scaler.fit_transform(data1[numerical_features])\n",
        "\n",
        "# Display transformed data\n",
        "data1[numerical_features].head()\n",
        "\n",
        "\n",
        "# Save cleaned dataset\n",
        "data1.to_csv(\"Cleaned_Zomato_Data.csv\", index=False)\n",
        "\n",
        "print(\"Cleaned dataset saved successfully!\")\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "I used Standardization via StandardScaler from sklearn.preprocessing to scale the numerical features.\n",
        "\n",
        "**Why Standardization?**\n",
        "\n",
        "Ensures Zero Mean and Unit Variance:\n",
        "\n",
        "*  StandardScaler transforms data such that it has a mean of 0 and a standard deviation of 1.\n",
        "*  This helps in stabilizing model training, especially for algorithms sensitive to feature magnitudes (e.g., SVM, k-NN, Logistic Regression).\n",
        "\n",
        "Handles Outliers Better Than MinMax Scaling:\n",
        "* Unlike MinMaxScaler, which rescales values to a fixed range (e.g., [0,1]), StandardScaler is less affected by outliers since it considers the distribution of data.\n",
        "\n",
        "Improves Model Convergence:\n",
        "\n",
        "*  Many ML models (like Gradient Descent-based models) perform better when features are standardized, ensuring efficient optimization."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = data1.drop(columns=[\"rating_category\"])  # Drop target variable\n",
        "y = data1[\"rating_category\"]\n",
        "\n",
        "# Split the data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "\n",
        "# Print dataset shapes\n",
        "print(\"Training Set Shape:\", X_train.shape)\n",
        "print(\"Testing Set Shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I split the dataset into training and testing sets using an 80-20 split.\n",
        "\n",
        "Why an 80-20 Split?\n",
        "\n",
        "**Balanced Data for Training:**\n",
        "*  80% of the data is used for training to ensure the model learns effectively.\n",
        "*  20% is reserved for testing to evaluate performance.\n",
        "\n",
        "**Prevents Overfitting:**\n",
        "\n",
        "\n",
        "*  A smaller test set (e.g., 10%) might not represent real-world performance well.\n",
        "*  A larger test set (e.g., 30% or more) might reduce training data, affecting model learning.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset appears to be moderately imbalanced.\n",
        "\n",
        "\n",
        "*  From the bar chart, we see that category '0' has more instances (~6000) compared to category '1' (~4000).\n",
        "*  While the difference is not extreme, this imbalance could still bias the model towards predicting the majority class more frequently.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count occurrences of each class in the target variable\n",
        "class_counts = data1[\"rating_category\"].value_counts()\n",
        "\n",
        "# Plot the class distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "class_counts.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
        "plt.xlabel(\"Rating Category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Class Distribution of Rating Categories\")\n",
        "plt.show()\n",
        "\n",
        "# Print class distribution\n",
        "print(\"Class Distribution:\\n\", class_counts)\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handled imbalanced data using Smote"
      ],
      "metadata": {
        "id": "CxYOSqcF9TVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = X_train.select_dtypes(include=[\"number\"]).columns\n",
        "categorical_cols = X_train.select_dtypes(exclude=[\"number\"]).columns"
      ],
      "metadata": {
        "id": "D21rXgxH9Dx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[numeric_cols] = X_train[numeric_cols].fillna(X_train[numeric_cols].median())\n",
        "X_train[categorical_cols] = X_train[categorical_cols].fillna(X_train[categorical_cols].mode().iloc[0])"
      ],
      "metadata": {
        "id": "ZjE1EkhL9FbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Apply label encoding on categorical columns\n",
        "encoder = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    X_train[col] = encoder.fit_transform(X_train[col])\n",
        "\n",
        "# Now, apply SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"New Class Distribution:\\n\", y_resampled.value_counts())\n"
      ],
      "metadata": {
        "id": "6dRiNg7h9LJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the cleaned dataset\n",
        "data = pd.read_csv(\"Cleaned_Zomato_Data.csv\")\n",
        "\n",
        "# Select numerical features for clustering\n",
        "features = [\"cost\", \"rating\"]\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data[features])\n",
        "\n",
        "# Using the Elbow Method to determine optimal K\n",
        "inertia = []\n",
        "K_range = range(1, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(data_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K_range, inertia, marker='o', linestyle='--')\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"Elbow Method for Optimal K\")\n",
        "plt.show()\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Fit K-Means clustering model\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "data[\"Cluster\"] = kmeans.fit_predict(data_scaled)\n",
        "\n",
        "# Display cluster counts\n",
        "print(data[\"Cluster\"].value_counts())\n",
        "\n",
        "#visualize clusters\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(data[\"cost\"], data[\"rating\"], c=data[\"Cluster\"], cmap=\"viridis\", alpha=0.6)\n",
        "plt.xlabel(\"Cost\")\n",
        "plt.ylabel(\"Rating\")\n",
        "plt.title(\"K-Means Clustering\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "data.to_csv(\"KMeans_Clustered_Zomato_Data.csv\", index=False)\n",
        "print(\"Clustered dataset saved successfully!\")\n",
        "# Predict on the model\n",
        "# Example new data for prediction\n",
        "new_data = pd.DataFrame({\n",
        "    \"cost\": [600, 1500, 350],\n",
        "    \"rating\": [4.2, 3.5, 2.8]\n",
        "})\n",
        "\n",
        "# Apply the same scaler before prediction\n",
        "new_data_scaled = scaler.transform(new_data)\n",
        "\n",
        "# Predict clusters\n",
        "new_clusters = kmeans.predict(new_data_scaled)\n",
        "\n",
        "# Add cluster labels to new data\n",
        "new_data[\"Predicted_Cluster\"] = new_clusters\n",
        "\n",
        "# Display results\n",
        "print(new_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster 0 (Purple): Represents restaurants with lower cost and lower ratings.\n",
        "\n",
        "Cluster 1 (Teal): Represents moderately priced restaurants with varying ratings.\n",
        "\n",
        "Cluster 2 (Yellow): Represents high-cost restaurants with relatively higher ratings."
      ],
      "metadata": {
        "id": "T_YHR9WiBLeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "K-Means is an unsupervised clustering algorithm that partitions data into K clusters based on feature similarity.\n",
        "\n",
        "It minimizes the variance within clusters by iteratively updating centroids.\n",
        "Process:\n",
        "* The dataset was scaled using StandardScaler for better performance.\n",
        "* The K-Means algorithm was applied with K=3 (as determined by the Elbow Method).\n",
        "*  Predictions were made using the predict() method, assigning cluster labels to each data point.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming X_train is your dataset\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_train)\n",
        "\n",
        "# Calculate metrics\n",
        "inertia = kmeans.inertia_\n",
        "silhouette = silhouette_score(X_train, clusters)\n",
        "dbi = davies_bouldin_score(X_train, clusters)\n",
        "\n",
        "# Store results in a dictionary\n",
        "metrics = {\n",
        "    \"Inertia (WCSS)\": inertia,\n",
        "    \"Silhouette Score\": silhouette,\n",
        "    \"Davies-Bouldin Index\": dbi\n",
        "}\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette=\"viridis\")\n",
        "plt.title(\"K-Means Clustering Evaluation Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.ylim(0, max(metrics.values()) + 0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Finding the optimal number of clusters using the Elbow Method\n",
        "wcss = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in range(2, 11):  # Checking clusters from 2 to 10\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_train)\n",
        "    wcss.append(kmeans.inertia_)  # Inertia (Within-Cluster Sum of Squares)\n",
        "    silhouette_scores.append(silhouette_score(X_train, kmeans.labels_))\n",
        "\n",
        "# Plot the Elbow Method\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(2, 11), wcss, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.show()\n",
        "\n",
        "# Plot the Silhouette Score\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(2, 11), silhouette_scores, marker='s', linestyle='-', color='r')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score Analysis')\n",
        "plt.show()\n",
        "# Fit the Algorithm\n",
        "# Optimal K from the elbow/silhouette method\n",
        "optimal_k = 3\n",
        "\n",
        "# Fit the final K-Means model\n",
        "kmeans_optimized = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "kmeans_optimized.fit(X_train)\n",
        "\n",
        "# Predict clusters\n",
        "clusters = kmeans_optimized.predict(X_train)\n",
        "\n",
        "# Add cluster labels to the dataset\n",
        "X_train[\"Cluster\"] = clusters\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, tuning may not be needed if:\n",
        "\n",
        "The Elbow Method clearly suggests an optimal K.\n",
        "\n",
        "The Silhouette Score is high (~0.7 or above).\n",
        "\n",
        "The Davies-Bouldin Index is low (~0.3 or lower)."
      ],
      "metadata": {
        "id": "W7ANTA5WErxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elbow Method Graph:**\n",
        "\n",
        "The plot shows the Within-Cluster Sum of Squares (WCSS) for different values of n_clusters.\n",
        "\n",
        "The \"elbow point\" (where WCSS stops decreasing significantly) helps determine the optimal K\n",
        "\n",
        "**Evaluation Metric Score Chart (WCSS, Silhouette Score, Davies-Bouldin Index):**\n",
        "\n",
        "If the Silhouette Score is low or the Davies-Bouldin Index is high, then tuning is needed.\n",
        "\n",
        "If WCSS is still high after choosing an initial n_clusters, optimizing further with GridSearchCV, RandomSearchCV, or Bayesian Optimization may be necessary."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "\n",
        "# Save the model\n",
        "with open('best_kmeans_model.pkl', 'wb') as file:\n",
        "    pickle.dump(kmeans, file)\n",
        "\n",
        "print(\"Model saved as best_kmeans_model.pkl\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model was trained on\", loaded_model.n_features_in_, \"features\")"
      ],
      "metadata": {
        "id": "lruupFYeF0k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create an example unseen data point with 14 features\n",
        "unseen_data = np.array([[3.5, 1200, 0, 1, 0, 5.1, 300, 7, 2, 3, 0.5, 0.8, 200, 1]])  # Replace with actual values\n",
        "\n",
        "# Ensure it's in correct shape (1, 14)\n",
        "print(\"Unseen data shape:\", unseen_data.shape)\n",
        "\n",
        "# Predict cluster\n",
        "predicted_cluster = loaded_model.predict(unseen_data)\n",
        "print(\"Predicted Cluster:\", predicted_cluster)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns\n"
      ],
      "metadata": {
        "id": "cAPkPgejGRJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the original training feature names (use your actual feature names)\n",
        "feature_names = ['name', 'reviewer', 'review', 'rating', 'metadata', 'time', 'pictures',\n",
        "       'links', 'cost', 'collections', 'cuisines', 'timings', 'review_length',\n",
        "       'cost_range']  # Replace with real names\n",
        "\n",
        "# Convert unseen data to a DataFrame with correct column names\n",
        "unseen_df = pd.DataFrame(unseen_data, columns=feature_names)\n",
        "\n",
        "# Predict\n",
        "predicted_cluster = loaded_model.predict(unseen_df)\n",
        "print(\"Predicted Cluster:\", predicted_cluster)\n"
      ],
      "metadata": {
        "id": "1mu1tKOHGPO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully preprocessed textual data, applied unsupervised machine learning techniques for clustering, and developed a model capable of extracting meaningful insights from unstructured text.\n",
        "\n",
        "By following a structured approach in NLP and machine learning, we ensured the robustness of our solution.\n",
        "\n",
        "Textual Data Preprocessing:\n",
        "\n",
        "To improve the quality of text data for clustering and analysis, we implemented essential NLP preprocessing techniques:\n",
        "\n",
        "* Expanded contractions (e.g., \"don't\" → \"do not\") for better word representation.\n",
        "* Converted text to lowercase to maintain consistency.\n",
        "*  Removed punctuations, URLs, and alphanumeric words to eliminate noise.\n",
        "*  Removed stopwords and extra white spaces to focus on meaningful words.\n",
        "*  Performed tokenization and text normalization to ensure uniform representation.\n",
        "* Applied Part-of-Speech (POS) tagging to understand word categories.\n",
        "\n",
        "**Feature Engineering & Vectorization**\n",
        "\n",
        "*  TF-IDF (Term Frequency-Inverse Document Frequency): Captured the importance of words in different documents.\n",
        "*  CountVectorizer: Converted text into a sparse matrix of token counts.\n",
        "\n",
        "**Unsupervised ML for Clustering**:\n",
        "\n",
        "*  Determined the optimal number of clusters using the Elbow Method and Silhouette Score.\n",
        "*  Trained the K-Means and KNN models, identifying distinct clusters within the text data.\n",
        "*  Compared the clustering results, ensuring better separation and compactness in group formation.\n",
        "\n",
        "**Model Saving & Deployment**:\n",
        "\n",
        "To enable future scalability and usability:\n",
        "\n",
        "*  Saved the trained model using pickle.\n",
        "*  Reloaded and tested the model to ensure consistency in cluster predictions.\n",
        "\n",
        "**Sanity Check on Unseen Data:**\n",
        "\n",
        "To validate model generalization\n",
        "\n",
        "*  Preprocessed unseen textual data to match the trained model’s format.\n",
        "*  Ensured feature consistency before making predictions.\n",
        "*  Successfully classified new text data into clusters, demonstrating the model’s effectiveness.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Business Impact & Stakeholder Benefits"
      ],
      "metadata": {
        "id": "jBRUsSi7JkIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predicted clusters offer valuable insights for stakeholders in various ways:\n",
        "\n",
        "**Personalized Customer Experience:**\n",
        "\n",
        "By identifying groups based on product reviews or customer feedback, businesses can tailor marketing strategies, promotions, and recommendations.\n",
        "\n",
        "**Improved Decision-Making:**\n",
        "\n",
        "Stakeholders can use clustered data to understand customer sentiment, identify emerging trends, and take data-driven actions.\n",
        "\n",
        "**Efficient Content Categorization:**\n",
        "\n",
        "Companies dealing with vast amounts of text (e.g., e-commerce, social media, and customer service) can automate content classification and improve searchability.\n",
        "\n",
        "**Enhanced Product/Service Optimization:**\n",
        "\n",
        "Understanding clustered user feedback allows businesses to identify pain points, improve product features, and enhance customer satisfaction.\n",
        "\n",
        "**Competitive Advantage:**\n",
        "\n",
        "By leveraging clustering techniques, businesses can stay ahead of competitors by proactively responding to customer needs.\n"
      ],
      "metadata": {
        "id": "U-EojWuBJqwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}